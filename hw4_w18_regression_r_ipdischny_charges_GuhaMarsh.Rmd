---
title: "HW4-Predictive regression modeling with R"
author: "misken"
date: "February 9, 2018"
output:
 html_document: 
   smart: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Step 1 - Familiarize yourself with the data and the assignment

In this assignment you'll build some predictive regression models
with R on a dataset containing inpatient discharges from hospitals in New York.

The version of
this data that we'll be using is from a Kaggle dataset. See
https://www.kaggle.com/jonasalmeida/2015-deidentified-ny-inpatient-discharge-sparcs. 
Unfortunately, the column metadata wasn't posted. However, since this is a
publicly available dataset, we can visit the source at 
https://health.data.ny.gov/Health/Hospital-Inpatient-Discharges-SPARCS-De-Identified/82xm-y6g8.

If you scroll down on that page you'll find descriptions of the columns (click
the little Show All link to display the entire list).

Most of the fields are self-explanatory. You'll notice that there are several
sets of diagnosis and procedure codes. A few definitions are helpful.

### DRG - Diagnosis Related Groups

DRGs are a coding system developed in the 1980s that form the basis of how
hospitals are reimbursed from Medicare (US Govt) or private insurers. After
a patient is discharged from the hospital, a program known as a *DRG grouper*
uses information such as diagnosis and procedure codes (ICD-9-CM) to assign a DRG
to the patient. A full list of the over 900 DRGs can be found at:

https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MedicareFeeforSvcPartsAB/downloads/DRGdesc08.pdf

In that list you'll see that MDC (Medical Diagnostic Category) is simply a
grouping of DRGs.

### CCS - Clinical Classification System

The [CCS](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) system was
developed by the [Agency for Healthcare Research and Quality
(AHRQ)](https://www.ahrq.gov/) to provide a classification system better suited
to healthcare research. There are CCS diagnosis codes and CCS procedure codes.
From their website:

> The Clinical Classifications Software (CCS) for ICD-9-CM is a diagnosis and
> procedure categorization scheme that can be employed in many types of projects
> analyzing data on diagnoses and procedures. CCS is based on the International
> Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM), a
> uniform and standardized coding system. The ICD-9-CM's multitude of codes - over
> 14,000 diagnosis codes and 3,900 procedure codes - are collapsed into a smaller
> number of clinically meaningful categories that are sometimes more useful for
> presenting descriptive statistics than are individual ICD-9-CM codes.


As we did in class, you'll be creating an R Markdown document to
do the analysis as well as to document the
steps you did (and answer some questions I'll throw at you).

You'll notice a few "Hacker Extra" tasks thrown in. These are for those of you
who want to go a little above and beyond and attempt some more challenging
tasks.

## Step 2 - Create a new R Markdown document

Save this file as a new R Markdown document and name it something that
includes your last name in the filename. Save it into the
same folder as this file.

## Step 3 - Create project and load data

Create an R Studio project in the current folder (the one containing this file). You'll notice that there is a folder named **data**.
Inside of it you'll find the data file for this assignment:

- **ipd_resp.RData**

The full dataset contains over two million records and is available as a CSV
file from Kaggle. I did a little data filtering and cleaning to create a 
subset to use for this regression assignment. Specifically, I did the following:

- used dplyr to filter records so that we were just working with `APR MDC Code` == 4. These are patients having respiratory related diagnoses.
- a bunch of fields were read in as `chr` and I changed them to factors using `as.factor`. 
- There were some fields we don't need and they were dropped.
- Just as in HW1, I cleaned up the charges and costs fields that
got interpreted as `chr` because of the leading dollar sign. Now they are 
numeric.
- Modified some field names to make them easier to work with. 

See the data prep script for all the details. You do **NOT** need to run the
data prep script. I'm just including it so you can see a typical data prep
script.

### Load the data

```{r explore_str}
#load("./data/ipd_resp.RData")

load('ipd_resp.RData')
#str(ipd_resp)
```
## Step 4 - Partition into training and test sets

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 30% of
the full dataset.

```{r partition}
set.seed(1828)
sample_size <- ceiling(0.30 * nrow(ipd_resp))
testrecs <- sample(nrow(ipd_resp),sample_size)
ipd_test <- ipd_resp[testrecs,]
ipd_train <- ipd_resp[-testrecs,]  # Negative in front of vector means "not in"
rm(ipd_resp) # No sense keeping a copy of the entire dataset around
```

## Step 5 - EDA on training data

Now start with some EDA on the training dataset`ipd_train`. The test data will only get
used after building models and want to compare their predictive abilities.

As mentioned above, the dependent variable that we are trying to predict is
`Total_Charges` - this is the amount that the hospital submits to whomever is
paying the bill for the hospital stay. This is usually an insurance company,
the federal Medicare or Medicaid program, an employer who self-insurers or
the patient. The `Payment_Typology_1` field contains the primary payer to whom
the charges are submitted. If you look at the relationship between `Total_Costs`
and `Total_Charges`, you'll start to see why the economics of the
US healthcare system is hard to understand.


You'll notice that `ipd_train` contains a few numeric fields and many
factors (categorical data). You are free to use any of these fields in your
regression models to predict `Total_Charges`.

Start by using things like ggplot2 and dplyr to explore the training dataset.
You can use other packages as well. Your goal is to gain a general understanding
of the variables and perhaps uncover some useful relationships that you can
use in your regression models.

**NOTE: In the data prep phase I made sure there were no NA values in the data
frames that we are using for this assignment. So, no need to worry about that.**

```{r}
library(dplyr)
library(ggplot2)
library(coefplot)
library(reshape2)
library(scales)
library(boot)
```


```{r}
summary(ipd_train)
```

Now, it's your turn ...
```{r initial ggplots}

#ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges))

# Histogram of Response/Output Variable 
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges), binwidth = 30000, fill = "steelblue", colour="Black") 

# Histogram of Predictor1/Input Numeric Variable1
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Costs), binwidth = 30000, fill = "orange", colour="Black")

# Histogram of Predictor2/Input Numeric Variable2
ggplot(data = ipd_train) + geom_histogram(aes(x = Length_of_Stay), fill = "violet", colour="Black")

# Scatterplots of Total_Charges vs Total_Costs
ggplot(data = ipd_train) + geom_point(aes(y = log(Total_Charges), x = log(Total_Costs))) 

# Scatterplots of Total_Charges vs Length_of_Stay
ggplot(data = ipd_train) + geom_point(aes(y = Total_Charges, x = Length_of_Stay))

# Scatterplots of Total_Charges vs Total_Costs faceted along Payment_Typology_1
ggplot(data = ipd_train) + geom_point(aes(y = Total_Charges, x = Total_Costs)) + 
  facet_wrap(~Payment_Typology_1)

# Scatterplots of Total_Charges vs Total_Costs faceted along Health_Service_Area
ggplot(data = ipd_train) + geom_point(aes(y = Total_Charges, x = Total_Costs, col = Health_Service_Area)) + 
  facet_wrap(~Health_Service_Area)

# Scatterplots of Total_Charges vs Total_Costs faceted along Age_Group
ggplot(data = ipd_train) + geom_point(aes(y = Total_Charges, x = Total_Costs, col = Age_Group)) + 
  facet_wrap(~Age_Group)

# Scatterplots of Total_Charges vs Total_Costs faceted along Type_of_Admission
ggplot(data = ipd_train) + geom_point(aes(y = Total_Charges, x = Total_Costs, col = Type_of_Admission)) + 
  facet_wrap(~Type_of_Admission)

# Histograms of Total_Charges along Patient_Disposition 
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges, binwidth = 30000, fill = Patient_Disposition)) + scale_x_log10()

# Histograms of Total_Charges along Hospital_County 
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges, binwidth = 3000000, fill = Hospital_County)) + scale_x_log10()

# Histograms of Total_Charges along APR_Severity_of_Illness_Desc  
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges, binwidth = 300000, fill = APR_Severity_of_Illness_Desc)) + scale_x_log10() 

# Histograms of Total_Charges along APR_Risk_of_Mortality
ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Charges, binwidth = 300000, fill = APR_Risk_of_Mortality)) + scale_x_log10()

#ggplot(data = ipd_train) + geom_histogram(aes(x = Total_Costs))

```


```{r}
# Printing only the Significant Co-efficients
# ss <- coef(summary(TotalCharges4))
# ss_sig <- ss[ss[,"Pr(>|t|)"]<0.05,]
# printCoefmat(ss_sig)
```



### Model Diagnostics
```{r}
# Coefficient Plot
#coefplot(TotalCharges6)

```


## Step 6 - Building and evaluation of predictive models

Now that you know a little more about the data, it's time to start building a
few predictive models for `Total_Charges`. The error metric that we will use
to evaluate the models (both for fit and for predictions) is 
**median absolute deviation** (MAD). This is nothing more than the median
of the absolute value of the residuals for a fitted model. 

**QUESTION:** What might be an advantage of using median absolute deviation instead
of mean absolute deviation?

**ANSWER:** The possible advantage of median absolute deviation over mean absolute 
deviation is that outliers have less of an affect on median absolute deviation. 

Obviously we want models with lower MAD values. Our ultimate goal is to create
a model that predicts well on the test dataset, `ipd_test`.

### Null model

It's always a good idea to start out with what we call the *null model*. This
is the simplest possible model and one that other models better be able to 
beat. For this regression problem, the null model is simply a regression model
that just has a y-intercept. If you remember some of your statistics, you won't
be surprised that the best fit value for the y-intercept in this case is 
the mean of the response variable, `Total_Charges`.

```{r charges_null_model}
charges_lm0 <- lm(Total_Charges ~ 1, data = ipd_train)
summary(charges_lm0)
# Compute the MAD value
median(abs(charges_lm0$residuals))
```

It's not hard to create regression models with smaller MAD values than this. :)

Just to give you a benchmark, I built a model on the training set that had
a MAD value of 7270.340. Again, this is just the median of the absolute
values of the residuals of the fitted model. Later when I used this model on
the test data, I got a MAD of ... well, I'll tell you a little later.


### EDA Prior to Model Building

```{r}
# Correlation Matrix
head(ipd_train)
str(ipd_train)
```


```{r}
# Correlation between Numeric Predictors
cor(ipd_train$Total_Charges, ipd_train$Total_Costs)
cor(ipd_train$Total_Charges, ipd_train$Length_of_Stay)
```

There seems to be a positive relationship between Total_Charges and Total_Costs and also between Total_Charges and the Length_of_Stay.

```{r}
# Selecting Just the Numeric Predictors to Build the Correlation Matrix
correlationbuild1 <- ipd_train %>%
  select(Facility_Name, Length_of_Stay, Total_Charges, Total_Costs)

head(correlationbuild1)
```

# Correlation Matrix
```{r}
# Correlation Matrix
cor(correlationbuild1[, c(2, 3:4)])

# Putting the Correlation Matrix in a Dataframe
ipdtrainCor <- cor(correlationbuild1[, c(2, 3:4)])

# Melting the Data for ease of plotting
correlationbuild1Melt <- melt(ipdtrainCor, varnames=c("x", "y"), value.name = "Correlation")

# Ordering the Correlation According to their Value
correlationbuild1Melt <- correlationbuild1Melt[order(correlationbuild1Melt$Correlation), ]

# Display the Melted Data
correlationbuild1Melt

# Heatmap of Correlation For Numerical Predictors
ggplot(correlationbuild1Melt, aes(x=x, y=y)) + geom_tile(aes(fill=Correlation)) + scale_fill_gradient2(low = muted("red"), mid = "white", high = "violet", guide = guide_colorbar(ticks = FALSE, barheight = 10), limits=c(-1, 1)) + theme_minimal() + labs(x=NULL, y = NULL)

```


### Model building

Using the information you've gained from reading about the data and doing some
EDA, build several regression models and compute the MAD from each. Summarize
your results in terms of which model or models appear to fit well. 



# Model 1: MAD: 7548.218
```{r}
# # Model 1: Build to Understand the effect of the Predictors on the Response Variable 
# totalCharges1 <- lm(Total_Charges ~ Total_Costs + Age_Group + Gender + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Patient_Disposition + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train)
# 
# summary(totalCharges1)
# 
# # Compute the MAD value
# median(abs(totalCharges1$residuals))
```

As, we can see that Gender is not significant. Let's try to fit our Model a little better, by removing the Predictor Gender

# Model 2: MAD: 7542.584
```{r}
# # Model 2: Build to Understand the effect of the Predictors on the Response Variable 
# totalCharges2 <- lm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Patient_Disposition + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train)
# 
# summary(totalCharges2)
# 
# # Compute the MAD value
# median(abs(totalCharges2$residuals))
```

Since, majority of the Co-efficients for the Patient_Disposition consists of an insignificant p-value. Let's try to fit the Model and check the MAD to check if removing the Predictor Patient_Disposition will be a good idea.  

# Model 3: MAD: 7534.875
```{r}
# # Model 3: Build to Understand the effect of the Predictors on the Response Variable 
# totalCharges3 <- lm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train)
# 
# summary(totalCharges3)
# 
# # Compute the MAD value
# median(abs(totalCharges3$residuals))
```

Yup, looks like MAD value did get reduced. Apparently our Model Fit just got better than the previous Model 2. 

# Model 4: MAD: 7608.123
```{r}
# # Model 4: Build to Understand the effect of the Predictors on the Response Variable, by removing APR_DRG_Desc for viewing the Correlation Matrix 
# totalCharges4 <- lm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train)
# 
# summary(totalCharges4)
# 
# # Compute the MAD value
# median(abs(totalCharges4$residuals))
```

In the above Model, looks like Fitting the Model upon removing the APR_DRG_Desc Predictors caused the Model MAD value to get high. So may be APR_DRG_Desc is a significant Predictor. Hence removing the Predictor APR_DRG_Desc might not be a good idea. So apparently our Model TotalCharges3 is still going strong. Let's try to build some more Models.

# Model 5: MAD: 7429.581
```{r}
# # Model 5: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & considering the Predictors  Total_Costs, Length_of_Stay, Health_Service_Area, Type_of_Admission, Payment_Typology_1, and Age_Group by removing the other Predictors for Model Fitting
# 
# totalCharges5 <- lm(Total_Charges ~ Total_Costs + Length_of_Stay + Health_Service_Area + Type_of_Admission + Payment_Typology_1 + Age_Group, data = ipd_train)
# 
# summary(totalCharges5)
# 
# # Compute the MAD value
# median(abs(totalCharges5$residuals))
# 
# # Model Visualization
# coefplot(totalCharges5)
# 
# # Model Visualization Considering Removing Some Insignificant Predictors
# coefplot(totalCharges5,predictors=c("Length_of_Stay","Total_Costs", "Health_Service_Area", "Type_of_Admission", "Payment_Typology_1"))
```

In terms of our Non-additive Models, this one looks to have the best fit, with the least MAD of all of the previous Models. From the Coefficient Plot some of the significant Predictors that might have influenced the Model parameters are Type_of_AdmissionTrauma which makes sense that if the Patient is getting admitted for trauma related factors then the Total_Charges would impact. Similarly the Total_Charges would also depend upon the location area of the health service area provider, such as Health_Service_AreaNew York City, Health_Service_AreaLong Island, Health_Service_AreaHudsonValley will have a significant impact on the Total_Charges.

## Additive Models 
# Model 6: MAD: 7516.532
```{r}
# # Model 6: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Length_of_Stay * Payment_Typology_1 for Model Fitting
# 
# totalCharges6 <- lm(Total_Charges ~ Total_Costs  + Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 + Type_of_Admission + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train)
# 
# summary(totalCharges6)
# 
# # Compute the MAD value
# median(abs(totalCharges6$residuals))
```

This is our first additive Model where we wanted to see the interaction between the patient's Length_of_Stay and the Payment_Typology_1 along with some other Predictors to see the Model significance. The MAD value got a significant reduction, from Model 4, but got higher from Model 5. Let's try to see if we can get any better with the MAD.

#  Model 7: MAD: 7245.633
```{r}
# # Model 7: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Length_of_Stay * Payment_Typology_1 * Total_Costs for Model Fitting. 
# 
# totalCharges7 <- lm(Total_Charges ~ Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 * Total_Costs +  Type_of_Admission + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train)
# 
# summary(totalCharges7)
# 
# # Compute the MAD value
# median(abs(totalCharges7$residuals))
```

In this Model we wanted to see the interaction amongst the patient's Length_of_Stay, Payment_Typology_1 which is nothing but the kind of Medical Insurance coverage along with the Total_Costs. So far our best fit Model with a MAD of 7245.633! That's pretty awesome. Let's see if we can bring the MAD further down by trying to fit some more Models.

# Model 8: MAD: 7285.403
```{r}
# # Model 8: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Length_of_Stay * Payment_Typology_1 * Total_Costs along with Type_of_Admission * APR_Severity_of_Illness_Desc for Model Fitting.
# 
# totalCharges8 <- lm(Total_Charges ~ Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 * Total_Costs + Type_of_Admission * APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train)
# 
# summary(totalCharges8)
# 
# # Compute the MAD value
# median(abs(totalCharges8$residuals))
```

In this Model we wanted to see the interaction between the patients' Type_of_Admission and APR_Severity_of_Illness_Desc along with other Predictors. However, our MAD got a little hike here, that we don't like. :( 
Let's try to build some more Models then!

# Model 9: MAD: 5932.954 (3rd Best Model)  
```{r}
# Model 9: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Health_Service_Area * Length_of_Stay * Total_Costs along with Type_of_Admission * Payment_Typology_1 for Model Fitting.

totalCharges9 <- lm(Total_Charges ~ Age_Group + Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission * Payment_Typology_1 + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train)

summary(totalCharges9)

# Compute the MAD value
median(abs(totalCharges9$residuals))
```

In this Model, we tried to see the interaction amongst the patients' Length_of_Stay, the Health_Service_Area, and the Total_Costs along with the interaction between the Type_of_Admission and the Payment_Typology_1. Some of the other Predictors that we considered for building this Models are patients' Age_Group, APR_Severity_of_Illness_Desc, and APR_Risk_of_Mortality.
Needless to say we got our another significant Model with a low MAD!

# Model 10: MAD: 5887.513 (2nd Best Model)
```{r}
# Model 10: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Health_Service_Area * Length_of_Stay * Total_Costs along with Payment_Typology_1 * Age_Group for Model Fitting

totalCharges10 <- lm(Total_Charges ~ Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission +  Payment_Typology_1 * Age_Group + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train)

summary(totalCharges10)

# Compute the MAD value
median(abs(totalCharges10$residuals))
```

Since most of the health insurance charges depends on patients' age, so we wanted to see the interaction between the Payment_Typology_1 * Age_Group. Not surprisingly we see that our Model Fit just got even more better, with the least MAD so far of about 5887.513.

# Model 11: MAD: 6138.078 
```{r}
# Model 11 (Best One So Far) : Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Health_Service_Area * Length_of_Stay for Model Fitting

totalCharges11 <- lm(Total_Charges ~ Total_Costs + Length_of_Stay * Health_Service_Area + Payment_Typology_1 + Age_Group + Type_of_Admission + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train)

summary(totalCharges11)

# Compute the MAD value
median(abs(totalCharges11$residuals))

# Model Visualization
coefplot(totalCharges11)

# Model Visualization Considering only the Numeric Predictors
coefplot(totalCharges11,predictors=c("Length_of_Stay","Total_Costs"))
```

In this Model we wanted to see the interaction between the Length_of_Stay * Health_Service_Area only without considering the interaction of the Total_Costs. Looks like not including Total_Costs, caused our Model to have a greater MAD than the previous Model 10.

# Model 12: MAD: 6121.064 
```{r}
# Model 12: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Health_Service_Area * Length_of_Stay and removing the Predictor APR_Risk_of_Mortality for Model Fitting

totalCharges12 <- lm(Total_Charges ~ Total_Costs + Length_of_Stay * Health_Service_Area + Payment_Typology_1 + Age_Group + Type_of_Admission + APR_Severity_of_Illness_Desc, data = ipd_train)

summary(totalCharges12)

# Compute the MAD value
median(abs(totalCharges12$residuals))
```

In this Model we tried to see the Model Fit by considering the interaction between Length_of_Stay and Health_Service_Area. Since from our previous EAD we saw that Length-of_Stay and Health_Service_Area are significant for the purpose of predicting the Total_Charges of patients. We also removed the Predictor APR_Risk_of_Mortality, since Total_Charges should not be much influenced by it. And our MAD value is still lower than Model 11. 

# Model 13: MAD: 5745.559 (1st Best Model)
```{r}
# Model 13: Build to Understand the effect of the Predictors on the Response Variable, by removing Race, Ethnicity, & introducing the interaction terms Total_Costs * Length_of_Stay * Health_Service_Area and Payment_Typology_1 * Type_of_Admission and by removing the other Predictors for Model Fitting

totalCharges13 <- lm(Total_Charges ~ Total_Costs * Length_of_Stay * Health_Service_Area + Payment_Typology_1 * Type_of_Admission + Age_Group, data = ipd_train)

summary(totalCharges13)

# Compute the MAD value
median(abs(totalCharges13$residuals))
```

Apparently Model 13 is our best Model with the least of MAD of about 5745.559! We assumed that the Total_Charges are dependent on the Length_of_Stay and the Health_Service_Area as well so we wanted to see the interaction amongst these terms. The other interactive terms that we wanted to see is the type of Insurance (Payment_Typology_1) and the reason for Admission (Type_of_Admission) as we can see that often Emergency units charge more than the Normal OPD Clinics.


Identify your top 3 models in terms of lowest MAD values to use in the next part in which you'll use them
to make predictions on the test dataset.

### My Top 3 Models are Model 13, Model 10 and Model 9 respectively


## Top 1 Model, MAD 5745.559
```{r}
# Top 1:- Model 13: MAD: 5745.559

# Model Coefficients
totalCharges13$coefficients

# Model Visualization
coefplot(totalCharges13)

# Model Visualization Considering Probable Significant Predictors
coefplot(totalCharges13,predictors=c("Length_of_Stay","Total_Costs"))

coefplot(totalCharges13,predictors=c("Health_Service_Area","Length_of_Stay"))
```

## Top 2 Model, MAD: 5887.513
```{r}
# Top 2:- Model 10: MAD: 5887.513 

# Model Coefficients
totalCharges10$coefficients

# Model Visualization
coefplot(totalCharges10)

# Model Visualization Considering Probable Significant Predictors
coefplot(totalCharges10,predictors=c("Length_of_Stay","Total_Costs"))

coefplot(totalCharges10,predictors=c("Health_Service_Area","Length_of_Stay"))
```


## Top 3 Model, MAD: 5932.954
```{r}
# Top 3:- Model 9: MAD: 5932.954

# Model Coefficients
totalCharges9$coefficients

# Model Visualization
coefplot(totalCharges9)

# Model Visualization Considering Probable Significant Predictors
coefplot(totalCharges9,predictors=c("Length_of_Stay","Total_Costs"))

coefplot(totalCharges9,predictors=c("Health_Service_Area","Length_of_Stay"))
```


**HACKER EXTRA:** Use k-crossfold validation to select your top 3 models.

## Crossfold Validation to Fit the Best Models
```{r}
# # Model 1
totalChargesG1 <- glm(Total_Charges ~ Total_Costs + Age_Group + Gender + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Patient_Disposition + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train, family=gaussian(link="identity"))
```

Let's check if this gives the same results as lm. No they don't!
```{r}
identical(coef(totalCharges9), coef(totalChargesG9))
```

Running the cross-validation with 5 folds
```{r}
totalChargesCV1 <- cv.glm(ipd_train, totalChargesG1, K=5)
```

Checking the Error
```{r}
totalChargesCV1$delta
```

# Refitting the other Models using glm
```{r}
# Model 2
totalChargesG2 <- glm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Patient_Disposition + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train, family=gaussian(link="identity"))

# Model 3
totalChargesG3 <- glm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc + APR_DRG_Desc, data = ipd_train, family=gaussian(link="identity"))


# Model 4
totalChargesG4 <- glm(Total_Charges ~ Total_Costs + Age_Group + Health_Service_Area + Race + Ethnicity + Length_of_Stay + Type_of_Admission + Payment_Typology_1 + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train, family=gaussian(link="identity"))


# Model 5
totalChargesG5 <- glm(Total_Charges ~ Total_Costs + Length_of_Stay + Health_Service_Area + Type_of_Admission + Payment_Typology_1 + Age_Group, data = ipd_train, family=gaussian(link="identity"))


# Model 6
totalChargesG6 <- glm(Total_Charges ~ Total_Costs  + Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 + Type_of_Admission + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train, family=gaussian(link="identity"))


# Model 7
totalChargesG7 <- glm(Total_Charges ~ Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 * Total_Costs +  Type_of_Admission + APR_Risk_of_Mortality + APR_Severity_of_Illness_Desc, data = ipd_train, family=gaussian(link="identity"))


# Model 8
totalChargesG8 <- glm(Total_Charges ~ Age_Group + Health_Service_Area + Length_of_Stay * Payment_Typology_1 * Total_Costs + Type_of_Admission * APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train, family=gaussian(link="identity"))


# Model 9
totalChargesG9 <- glm(Total_Charges ~ Age_Group + Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission * Payment_Typology_1 + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train, family=gaussian(link="identity"))


# Model 10
totalChargesG10 <- glm(Total_Charges ~ Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission +  Payment_Typology_1 * Age_Group + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train, family=gaussian(link="identity"))


# Model 11
totalChargesG11 <- glm(Total_Charges ~ Total_Costs + Length_of_Stay * Health_Service_Area + Payment_Typology_1 + Age_Group + Type_of_Admission + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_train, family=gaussian(link="identity"))


# Model 12
totalChargesG12 <- glm(Total_Charges ~ Total_Costs + Length_of_Stay * Health_Service_Area + Payment_Typology_1 + Age_Group + Type_of_Admission + APR_Severity_of_Illness_Desc, data = ipd_train, family=gaussian(link="identity"))


# Model 13
totalChargesG13 <- glm(Total_Charges ~ Total_Costs * Length_of_Stay * Health_Service_Area + Payment_Typology_1 * Type_of_Admission + Age_Group, data = ipd_train, family=gaussian(link="identity"))

```

# Running Cross-Validation
```{r}
totalChargesCV2 <- cv.glm(ipd_train, totalChargesG2, K=5)
totalChargesCV3 <- cv.glm(ipd_train, totalChargesG3, K=5)
totalChargesCV4 <- cv.glm(ipd_train, totalChargesG4, K=5)
totalChargesCV5 <- cv.glm(ipd_train, totalChargesG5, K=5)
totalChargesCV6 <- cv.glm(ipd_train, totalChargesG6, K=5)
totalChargesCV7 <- cv.glm(ipd_train, totalChargesG7, K=5)
totalChargesCV8 <- cv.glm(ipd_train, totalChargesG8, K=5)
totalChargesCV9 <- cv.glm(ipd_train, totalChargesG9, K=5)
totalChargesCV10 <- cv.glm(ipd_train, totalChargesG10, K=5)
totalChargesCV11 <- cv.glm(ipd_train, totalChargesG11, K=5)
totalChargesCV12 <- cv.glm(ipd_train, totalChargesG12, K=5)
totalChargesCV13 <- cv.glm(ipd_train, totalChargesG13, K=5)
```


```{r}

```


### Model diagnostics


#### Scatterplots of actual vs fitted values

For your top 3 models, create a scatter plot showing actual vs fitted values
of `Total_Charges`. Remember, it's often nice to "gather up" your results
into a data frame to facilitate plotting. See the notes on comparing competing
regression models. Here's what one of my scatterplots looks like.

```{r}
# modelTests <- data.frame(ActualCharges=ipd_train[,"Total_Charges"],
#                           LM13_TotalCharges=totalCharges13$fit[,1],
#                           LM10_TotalCharges=totalCharges10$fit[,1],
#                           LM9_TotalCharges=totalCharges9$fit[,1])

modelTests <- data.frame(New_ActualCharges=ipd_train[,"Total_Charges"],
                          LM13_TotalCharges=predict(totalCharges13),
                          LM10_TotalCharges=predict(totalCharges10),
                          LM9_TotalCharges=predict(totalCharges9))


modelTests 


ggplot(data=modelTests, aes(x=Total_Charges,y=LM13_TotalCharges)) + geom_point()
ggplot(data=modelTests, aes(x=Total_Charges,y=LM10_TotalCharges)) + geom_point()
ggplot(data=modelTests, aes(x=Total_Charges,y=LM9_TotalCharges)) + geom_point()


```


![](./scatter_lm1.png)

#### Normality of residuals

Create a histogram of the residuals for your top 3 models. You may have to
tweak the axes scales to make a nice histogram.

#### Constant variance

Make an appropriate plot to check for constant variance (homeskedasticity) for
your top model. 
Don't remember what kind of plot to make? See my notes on residual analysis
or any intro stats book.

### Make predictions for the test dataset

For each of your top 3 models, make predictions for `ipd_test`.

```{r}
# Making Prediction with Test Data and 95% Confidence Interval
totalChargesPredict1 <- predict(totalCharges13, newdata=ipd_test, se.fit=TRUE,
                         interval="prediction", level=0.95)

totalChargesPredict2 <- predict(totalCharges10, newdata=ipd_test, se.fit=TRUE,
                         interval="prediction", level=0.95)

totalChargesPredict3 <- predict(totalCharges9, newdata=ipd_test, se.fit=TRUE,
                         interval="prediction", level=0.95)


# Save relevant objects
save(ipd_train,ipd_test,totalCharges13,totalCharges10,totalCharges9,
     totalChargesPredict1,totalChargesPredict2,totalChargesPredict3,
     file="totalChargesPredict123.rdata")

```



### Evaluate the predictions

Compute the MAD for each of the three models' predictions on the test data.

HINT: Obviously you can't use the residuals (or errors) directly but will have to compute
them. They are simply the difference between the actual values and your predicted values.

So, for the model I mentioned earlier that had a MAD of 7270.340 for the training
data, the MAD on the test data was 7335.559. Notice that MAD for test is 
higher than MAD for train.

## Top Predictive Models
# Top 1 Predictive Model, MAD: 5771.55
```{r}
# Top 1 Predictive Model, based on Model 13 of Train Data
totalChargesPredict13 <- lm(Total_Charges ~ Total_Costs * Length_of_Stay * Health_Service_Area + Payment_Typology_1 * Type_of_Admission + Age_Group, data = ipd_test)

summary(totalChargesPredict13)

# Compute the MAD value
median(abs(totalChargesPredict13$residuals))
```


# Top 2 Predictive Model, MAD: 5839.624
```{r}
# Top 2 Predictive Model, based on Model 10 of Train Data
totalChargesPredict10 <- lm(Total_Charges ~ Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission +  Payment_Typology_1 * Age_Group + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_test)

summary(totalChargesPredict10)

# Compute the MAD value
median(abs(totalChargesPredict10$residuals))
```


# Top 3 Predictive Model, MAD: 5922.874
```{r}
# Top 3 Predictive Model, based on Model 9 of Train Data
totalChargesPredict9 <- lm(Total_Charges ~ Age_Group + Health_Service_Area * Length_of_Stay * Total_Costs + Type_of_Admission * Payment_Typology_1 + APR_Severity_of_Illness_Desc + APR_Risk_of_Mortality, data = ipd_test)

summary(totalChargesPredict9)

# Compute the MAD value
median(abs(totalChargesPredict9$residuals))
```



**QUESTION:** Do you think it's typical that MAD for test would be higher than
MAD for train? Why or why not?

**QUESTION:** Which of your top 3 models had the lowest MAD for the test data? 

**QUESTION:** How do the MAD values for the test data compare to the MAD values
for the training data?


Finally, create some sort of plot which shows the MAD values for both 
train and test for your top 3 models. One plot.

### Your top model

Show your top performing model and discuss whether the model appears to make
sense in terms of the variables included. Why did you choose the variables you
did?

It will be interesting to compare the best models that everyone finds.

Later we'll learn more techniques that will likely allow us to beat simple
linear models.

